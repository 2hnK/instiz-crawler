"""
Instiz ì‹œê°„ëŒ€ë³„ ê· ë“± ìƒ˜í”Œë§ í¬ë¡¤ëŸ¬
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
from urllib.parse import quote

class InstizTimeSampler:
    """ì‹œê°„ëŒ€ë³„ ê· ë“± ìƒ˜í”Œë§ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, target_per_timerange=1250):
        self.base_url = "https://www.instiz.net/name"
        self.target_per_timerange = target_per_timerange
        
        # 4ê°œ ì‹œê°„ëŒ€ (6ì‹œê°„ì”©)
        self.time_ranges = [
            ('00:00', '05:59', 'ìƒˆë²½'),
            ('06:00', '11:59', 'ì˜¤ì „'),
            ('12:00', '17:59', 'ì˜¤í›„'),
            ('18:00', '23:59', 'ì €ë…')
        ]
    
    def build_url(self, year, month, start_time, end_time, page=1):
        """
        URL ìƒì„±
        
        Args:
            year: 2024
            month: 1
            start_time: '00:00'
            end_time: '05:59'
            page: í˜ì´ì§€ ë²ˆí˜¸
        """
        # ë‚ ì§œ ë²”ìœ„
        start_date = f"{year}/{month:02d}/01 {start_time}"
        
        # í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ ë‚  ê³„ì‚°
        if month == 12:
            next_month = datetime(year + 1, 1, 1)
        else:
            next_month = datetime(year, month + 1, 1)
        last_day = (next_month - timedelta(days=1)).day
        end_date = f"{year}/{month:02d}/{last_day} {end_time}"
        
        # URL ì¸ì½”ë”©
        start_encoded = quote(start_date)
        end_encoded = quote(end_date)
        
        url = (
            f"{self.base_url}?"
            f"page={page}&"
            f"category=1&"  # ì¼ìƒ ê²Œì‹œíŒ
            f"k=%EA%B8%B0%EA%B0%84%ED%83%90%EC%83%89&"  # ê¸°ê°„íƒìƒ‰
            f"stype=9&"
            f"starttime={start_encoded}&"
            f"endtime={end_encoded}"
        )
        
        return url
    
    def get_total_posts_in_range(self, year, month, start_time, end_time):
        """
        íŠ¹ì • ì‹œê°„ëŒ€ì˜ ì „ì²´ ê²Œì‹œë¬¼ ìˆ˜ í™•ì¸
        
        Returns:
            total_posts: ì „ì²´ ê²Œì‹œë¬¼ ìˆ˜
            total_pages: ì „ì²´ í˜ì´ì§€ ìˆ˜
        """
        url = self.build_url(year, month, start_time, end_time, page=1)
        
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ì „ì²´ ê²Œì‹œë¬¼ ìˆ˜ ì¶”ì¶œ
            # (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            pagination = soup.find('div', class_='pagination')
            if pagination:
                # ì˜ˆì‹œ: "1 / 2500" í˜•íƒœ
                total_posts = int(pagination.text.split('/')[-1].strip())
                total_pages = (total_posts // 20) + (1 if total_posts % 20 else 0)
                return total_posts, total_pages
            
        except Exception as e:
            print(f"Error: {e}")
            return None, None
    
    def calculate_sampling_pages(self, total_pages, target_count):
        """
        ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§í•  í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚°
        
        Args:
            total_pages: ì „ì²´ í˜ì´ì§€ ìˆ˜
            target_count: ëª©í‘œ ê²Œì‹œë¬¼ ìˆ˜
        
        Returns:
            list of page numbers to crawl
        """
        posts_per_page = 20
        target_pages = target_count // posts_per_page
        
        if target_pages >= total_pages:
            # ëª©í‘œê°€ ì „ì²´ë³´ë‹¤ ë§ìœ¼ë©´ ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘
            return list(range(1, total_pages + 1))
        
        # ê· ë“± ê°„ê²© ê³„ì‚°
        interval = total_pages / target_pages
        pages = [int(i * interval) + 1 for i in range(target_pages)]
        
        return pages
    
    def crawl_page(self, url):
        """
        ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§
        
        Returns:
            list of dict: ê²Œì‹œë¬¼ ì •ë³´
        """
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            posts = []
            # ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
            post_elements = soup.find_all('div', class_='post-item')
            
            for post in post_elements:
                post_data = {
                    'id': post.get('data-id'),
                    'url': post.find('a')['href'],
                    'title': post.find('h3').text.strip(),
                    'body': post.find('div', class_='body').text.strip(),
                    'created_at': post.find('time')['datetime'],
                    'likes': int(post.find('span', class_='likes').text),
                    'comments_count': int(post.find('span', class_='comments').text)
                }
                posts.append(post_data)
            
            return posts
            
        except Exception as e:
            print(f"Error crawling page: {e}")
            return []
    
    def sample_month(self, year, month):
        """
        í•œ ë‹¬ ì „ì²´ë¥¼ ì‹œê°„ëŒ€ë³„ ê· ë“± ìƒ˜í”Œë§
        
        Args:
            year: 2024
            month: 1
        
        Returns:
            DataFrame with sampled posts
        """
        all_posts = []
        
        print(f"\n{'='*60}")
        print(f"ìˆ˜ì§‘ ì‹œì‘: {year}ë…„ {month}ì›”")
        print(f"{'='*60}")
        
        for start_time, end_time, label in self.time_ranges:
            print(f"\nğŸ“… {label} ({start_time}-{end_time}) ìˆ˜ì§‘ ì¤‘...")
            
            # 1. ì „ì²´ ê²Œì‹œë¬¼ ìˆ˜ í™•ì¸
            total_posts, total_pages = self.get_total_posts_in_range(
                year, month, start_time, end_time
            )
            
            if total_posts is None:
                print(f"   âš ï¸ ê²Œì‹œë¬¼ ìˆ˜ í™•ì¸ ì‹¤íŒ¨")
                continue
            
            print(f"   ì „ì²´ ê²Œì‹œë¬¼: {total_posts:,}ê°œ ({total_pages:,} í˜ì´ì§€)")
            
            # 2. ìƒ˜í”Œë§í•  í˜ì´ì§€ ê³„ì‚°
            pages_to_crawl = self.calculate_sampling_pages(
                total_pages, 
                self.target_per_timerange
            )
            
            print(f"   ìˆ˜ì§‘ ëª©í‘œ: {self.target_per_timerange:,}ê°œ")
            print(f"   í¬ë¡¤ë§ í˜ì´ì§€: {len(pages_to_crawl):,}ê°œ")
            
            # 3. í˜ì´ì§€ë³„ í¬ë¡¤ë§
            for i, page_num in enumerate(pages_to_crawl):
                if i > 0 and i % 10 == 0:
                    print(f"   ì§„í–‰: {i}/{len(pages_to_crawl)} í˜ì´ì§€...")
                
                url = self.build_url(year, month, start_time, end_time, page_num)
                posts = self.crawl_page(url)
                all_posts.extend(posts)
                
                # Rate limiting
                time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
            
            print(f"   âœ… {label} ì™„ë£Œ: {len([p for p in all_posts if p.get('timerange')==label]):,}ê°œ ìˆ˜ì§‘")
        
        # DataFrame ë³€í™˜
        df = pd.DataFrame(all_posts)
        
        print(f"\n{'='*60}")
        print(f"ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ì´ ê²Œì‹œë¬¼: {len(df):,}ê°œ")
        print(f"ì‹œê°„ëŒ€ë³„ ë¶„í¬:")
        for _, _, label in self.time_ranges:
            count = len([p for p in all_posts if p.get('timerange') == label])
            print(f"   {label}: {count:,}ê°œ")
        print(f"{'='*60}\n")
        
        return df
    
    def sample_year(self, year, months=range(1, 13)):
        """
        1ë…„ ì „ì²´ ìˆ˜ì§‘
        
        Args:
            year: 2024
            months: ìˆ˜ì§‘í•  ì›” ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 1-12ì›”)
        
        Returns:
            DataFrame
        """
        all_data = []
        
        for month in months:
            print(f"\n{'#'*60}")
            print(f"# {year}ë…„ {month}ì›” ìˆ˜ì§‘")
            print(f"{'#'*60}")
            
            monthly_data = self.sample_month(year, month)
            all_data.append(monthly_data)
            
            # ì›”ë³„ ì €ì¥ (ë°±ì—…)
            filename = f"instiz_{year}-{month:02d}_sampled.csv"
            monthly_data.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ì €ì¥: {filename}")
            
            # ë‹¤ìŒ ì›”ë¡œ ë„˜ì–´ê°€ê¸° ì „ ëŒ€ê¸°
            time.sleep(5)
        
        # ì „ì²´ ë°ì´í„° í†µí•©
        full_data = pd.concat(all_data, ignore_index=True)
        
        # ìµœì¢… ì €ì¥
        final_filename = f"instiz_{year}_full_sampled.csv"
        full_data.to_csv(final_filename, index=False, encoding='utf-8-sig')
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ì´ ê²Œì‹œë¬¼: {len(full_data):,}ê°œ")
        print(f"ì €ì¥ íŒŒì¼: {final_filename}")
        print(f"{'='*60}")
        
        return full_data


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (ì‹œê°„ëŒ€ë‹¹ 1,250ê°œì”©)
    sampler = InstizTimeSampler(target_per_timerange=1250)
    
    # ì˜µì…˜ 1: ë‹¨ì¼ ì›” ìˆ˜ì§‘
    # data = sampler.sample_month(2024, 1)
    
    # ì˜µì…˜ 2: 1ë…„ ì „ì²´ ìˆ˜ì§‘
    data = sampler.sample_year(2024)
    
    # ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
    print("\nğŸ“Š ìˆ˜ì§‘ ë°ì´í„° ìš”ì•½:")
    print(data.info())
    print("\nì‹œê°„ëŒ€ë³„ ë¶„í¬:")
    print(data.groupby('timerange').size())
